{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32) Tensor(\"Const_1:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "node1 = tf.constant(3.0, dtype=tf.float32)\n",
    "node2 = tf.constant(4.0) # also tf.float32 implicitly\n",
    "print(node1, node2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.0, 4.0]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "print(sess.run([node1, node2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node3: Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node3 = tf.add(node1, node2)\n",
    "print(\"node3:\", node3)\n",
    "#print(\"sess.run(node3):\", sess.run(node3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node3: Tensor(\"Add_1:0\", shape=(), dtype=float32)\n",
      "sess.run(node3): 7.0\n"
     ]
    }
   ],
   "source": [
    "node3 = tf.add(node1, node2)\n",
    "print(\"node3:\", node3)\n",
    "print(\"sess.run(node3):\", sess.run(node3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to run the session before anything happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "adder_node = a + b  # + provides a shortcut for tf.add(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(adder_node, {a: 3, b: 4.5}))\n",
    "print(sess.run(adder_node, {a: [1, 3], b: [2, 4]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.  7.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(adder_node, {a: [1, 3], b: [2, 4]}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  5.  4.  7.  2.  3.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(adder_node, {a: ([0, 1, 1, 3,3, 1, 3, 0]), b: [2, 3, 4, 1, 4, 1, 0, 0]}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Can do this with arrays as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  4.  5.  4.]\n",
      " [ 7.  2.  3.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(adder_node, {a: [[0, 1, 1, 3],[3, 1, 3, 0]], b: [[2, 3, 4, 1], [4, 1, 0, 0]]}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 80 564 290 563  77  38 885 333 715 393]\n"
     ]
    }
   ],
   "source": [
    "import numpy.random as nprnd\n",
    "g = nprnd.randint(1000, size=10)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add another node\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "add_and_triple = adder_node * 3.\n",
    "print(sess.run(add_and_triple, {a: 3, b: 4.5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants are initialized when you call tf.constant, and their value can never change. By contrast, variables are not initialized when you call tf.Variable. To initialize all the variables in a TensorFlow program, you must explicitly call a special operation as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.30000001  0.60000002  0.90000004]\n",
      "{'W': array([ 0.30000001], dtype=float32)}\n",
      "{'b': array([-0.30000001], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(linear_model, {x: [1, 2, 3, 4]}))\n",
    "print(sess.run({'W': W}))\n",
    "\n",
    "print(sess.run({'b': b}))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.66\n"
     ]
    }
   ],
   "source": [
    "y = tf.placeholder(tf.float32)\n",
    "squared_deltas = tf.square(linear_model - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "print(sess.run(loss, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could improve this manually by reassigning the values of W and b to the perfect values of -1 and 1. \n",
    "A variable is initialized to the value provided to tf.Variable but can be changed using operations like tf.assign. For example, W=-1 and b=1 are the optimal parameters for our model. \n",
    "\n",
    "We can change W and b accordingly:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "fixW = tf.assign(W, [-1.])\n",
    "fixb = tf.assign(b, [1.])\n",
    "sess.run([fixW, fixb])\n",
    "print(sess.run(loss, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow provides optimizers that slowly change each variable in order to minimize the loss function. \n",
    "The simplest optimizer is **gradient descent**. \n",
    "\n",
    "It modifies each variable according to the magnitude of the derivative of loss with respect to that variable. In general, computing symbolic derivatives manually is tedious and error-prone. \n",
    "\n",
    "TensorFlow can automatically produce derivatives given only a description of the model using the function tf.gradients. For simplicity, optimizers typically do this for you. For example,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-0.9999969], dtype=float32), array([ 0.99999082], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "sess.run(init) # reset values to incorrect defaults.\n",
    "for i in range(1000):\n",
    "  sess.run(train, {x: [1, 2, 3, 4], y: [0, -1, -2, -3]})\n",
    "\n",
    "print(sess.run([W, b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete program for Linear Regression Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W: [-0.9999969] b: [ 0.99999082] loss: 5.69997e-11\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Model parameters\n",
    "W = tf.Variable([.3], dtype=tf.float32)\n",
    "b = tf.Variable([-.3], dtype=tf.float32)\n",
    "# Model input and output\n",
    "x = tf.placeholder(tf.float32)\n",
    "linear_model = W * x + b\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# loss\n",
    "loss = tf.reduce_sum(tf.square(linear_model - y)) # sum of the squares\n",
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# training data\n",
    "x_train = [1, 2, 3, 4]\n",
    "y_train = [0, -1, -2, -3]\n",
    "# training loop\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init) # reset values to wrong\n",
    "for i in range(1000):\n",
    "  sess.run(train, {x: x_train, y: y_train})\n",
    "\n",
    "# evaluate training accuracy\n",
    "curr_W, curr_b, curr_loss = sess.run([W, b, loss], {x: x_train, y: y_train})\n",
    "print(\"W: %s b: %s loss: %s\"%(curr_W, curr_b, curr_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "https://www.tensorflow.org/get_started/get_started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf.estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** tf.estimator ** is a high-level TensorFlow library that simplifies the mechanics of machine learning, including the following:\n",
    "\n",
    "running training loops\n",
    "\n",
    "running evaluation loops\n",
    "\n",
    "managing data sets\n",
    "\n",
    "\n",
    "tf.estimator defines many common models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/66/7txfr8_j2wn5skfx0jbnm3300000gn/T/tmp7u62kacm\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/66/7txfr8_j2wn5skfx0jbnm3300000gn/T/tmp7u62kacm', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/66/7txfr8_j2wn5skfx0jbnm3300000gn/T/tmp7u62kacm/model.ckpt.\n",
      "INFO:tensorflow:loss = 9.0, step = 1\n",
      "INFO:tensorflow:global_step/sec: 512.233\n",
      "INFO:tensorflow:loss = 0.0800316, step = 101 (0.197 sec)\n",
      "INFO:tensorflow:global_step/sec: 596.736\n",
      "INFO:tensorflow:loss = 0.0138438, step = 201 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 632.136\n",
      "INFO:tensorflow:loss = 0.00035158, step = 301 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 834.242\n",
      "INFO:tensorflow:loss = 0.000103384, step = 401 (0.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 791.879\n",
      "INFO:tensorflow:loss = 1.50967e-05, step = 501 (0.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 794.452\n",
      "INFO:tensorflow:loss = 7.54977e-07, step = 601 (0.127 sec)\n",
      "INFO:tensorflow:global_step/sec: 930.622\n",
      "INFO:tensorflow:loss = 2.33689e-07, step = 701 (0.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 788.737\n",
      "INFO:tensorflow:loss = 3.52719e-08, step = 801 (0.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 769.36\n",
      "INFO:tensorflow:loss = 2.53427e-09, step = 901 (0.128 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/66/7txfr8_j2wn5skfx0jbnm3300000gn/T/tmp7u62kacm/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.46862e-10.\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-07-18:36:13\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/66/7txfr8_j2wn5skfx0jbnm3300000gn/T/tmp7u62kacm/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-07-18:36:14\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 6.47164e-11, global_step = 1000, loss = 2.58866e-10\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-07-18:36:15\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/66/7txfr8_j2wn5skfx0jbnm3300000gn/T/tmp7u62kacm/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-07-18:36:16\n",
      "INFO:tensorflow:Saving dict for global step 1000: average_loss = 0.00252566, global_step = 1000, loss = 0.0101026\n",
      "train metrics: {'average_loss': 6.4716441e-11, 'loss': 2.5886576e-10, 'global_step': 1000}\n",
      "eval metrics: {'average_loss': 0.0025256618, 'loss': 0.010102647, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# NumPy is often used to load, manipulate and preprocess data.\n",
    "import numpy as np\n",
    "\n",
    "# Declare list of features. We only have one numeric feature. There are many\n",
    "# other types of columns that are more complicated and useful.\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[1])]\n",
    "\n",
    "# An estimator is the front end to invoke training (fitting) and evaluation\n",
    "# (inference). There are many predefined types like linear regression,\n",
    "# linear classification, and many neural network classifiers and regressors.\n",
    "# The following code provides an estimator that does linear regression.\n",
    "estimator = tf.estimator.LinearRegressor(feature_columns=feature_columns)\n",
    "\n",
    "# TensorFlow provides many helper methods to read and set up data sets.\n",
    "# Here we use two data sets: one for training and one for evaluation\n",
    "# We have to tell the function how many batches\n",
    "# of data (num_epochs) we want and how big each batch should be.\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "\n",
    "# We can invoke 1000 training steps by invoking the  method and passing the\n",
    "# training data set.\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "\n",
    "# Here we evaluate how well our model did.\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_fn)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train metrics: %r\"% train_metrics)\n",
    "print(\"eval metrics: %r\"% eval_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will work on making plots later\n",
    "\n",
    "plt.plot(np.squeeze(costs))\n",
    "\n",
    "plt.ylabel('cost')\n",
    "\n",
    "plt.xlabel('iterations (per tens)')\n",
    "\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a Custom Model using lower level functions of tf.estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/66/7txfr8_j2wn5skfx0jbnm3300000gn/T/tmpiuabtvxt\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/66/7txfr8_j2wn5skfx0jbnm3300000gn/T/tmpiuabtvxt', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/66/7txfr8_j2wn5skfx0jbnm3300000gn/T/tmpiuabtvxt/model.ckpt.\n",
      "INFO:tensorflow:loss = 9.85315286822, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1161.94\n",
      "INFO:tensorflow:loss = 0.0235151029472, step = 101 (0.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 1205.15\n",
      "INFO:tensorflow:loss = 0.00330104600997, step = 201 (0.084 sec)\n",
      "INFO:tensorflow:global_step/sec: 1029.22\n",
      "INFO:tensorflow:loss = 0.00022648098187, step = 301 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 1122.56\n",
      "INFO:tensorflow:loss = 3.30449501539e-05, step = 401 (0.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 1000.7\n",
      "INFO:tensorflow:loss = 1.96629135185e-06, step = 501 (0.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 970.555\n",
      "INFO:tensorflow:loss = 3.80164987391e-08, step = 601 (0.102 sec)\n",
      "INFO:tensorflow:global_step/sec: 468.105\n",
      "INFO:tensorflow:loss = 6.34259363668e-09, step = 701 (0.216 sec)\n",
      "INFO:tensorflow:global_step/sec: 379.697\n",
      "INFO:tensorflow:loss = 1.64128807521e-09, step = 801 (0.260 sec)\n",
      "INFO:tensorflow:global_step/sec: 626.888\n",
      "INFO:tensorflow:loss = 1.47520580315e-10, step = 901 (0.159 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/66/7txfr8_j2wn5skfx0jbnm3300000gn/T/tmpiuabtvxt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.36693559152e-11.\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-07-18:38:56\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/66/7txfr8_j2wn5skfx0jbnm3300000gn/T/tmpiuabtvxt/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-07-18:38:57\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 1.19711e-11\n",
      "INFO:tensorflow:Starting evaluation at 2017-09-07-18:38:57\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/66/7txfr8_j2wn5skfx0jbnm3300000gn/T/tmpiuabtvxt/model.ckpt-1000\n",
      "INFO:tensorflow:Finished evaluation at 2017-09-07-18:38:58\n",
      "INFO:tensorflow:Saving dict for global step 1000: global_step = 1000, loss = 0.0101004\n",
      "train metrics: {'loss': 1.1971128e-11, 'global_step': 1000}\n",
      "eval metrics: {'loss': 0.010100371, 'global_step': 1000}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Declare list of features, we only have one real-valued feature\n",
    "def model_fn(features, labels, mode):\n",
    "  # Build a linear model and predict values\n",
    "  W = tf.get_variable(\"W\", [1], dtype=tf.float64)\n",
    "  b = tf.get_variable(\"b\", [1], dtype=tf.float64)\n",
    "  y = W * features['x'] + b\n",
    "  # Loss sub-graph\n",
    "  loss = tf.reduce_sum(tf.square(y - labels))\n",
    "  # Training sub-graph\n",
    "  global_step = tf.train.get_global_step()\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "  train = tf.group(optimizer.minimize(loss),\n",
    "                   tf.assign_add(global_step, 1))\n",
    "  # EstimatorSpec connects subgraphs we built to the\n",
    "  # appropriate functionality.\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=y,\n",
    "      loss=loss,\n",
    "      train_op=train)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn)\n",
    "# define our data sets\n",
    "x_train = np.array([1., 2., 3., 4.])\n",
    "y_train = np.array([0., -1., -2., -3.])\n",
    "x_eval = np.array([2., 5., 8., 1.])\n",
    "y_eval = np.array([-1.01, -4.1, -7, 0.])\n",
    "input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=None, shuffle=True)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_train}, y_train, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    {\"x\": x_eval}, y_eval, batch_size=4, num_epochs=1000, shuffle=False)\n",
    "\n",
    "# train\n",
    "estimator.train(input_fn=input_fn, steps=1000)\n",
    "# Here we evaluate how well our model did.\n",
    "train_metrics = estimator.evaluate(input_fn=train_input_fn)\n",
    "eval_metrics = estimator.evaluate(input_fn=eval_input_fn)\n",
    "print(\"train metrics: %r\"% train_metrics)\n",
    "print(\"eval metrics: %r\"% eval_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax reference\n",
    "http://neuralnetworksanddeeplearning.com/chap3.html#softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "#x isn't a specific value. \n",
    "#It's a placeholder, a value that we'll input when we ask TensorFlow to run a computation. \n",
    "#We want to be able to input any number of MNIST images, each flattened into a 784-dimensional vector. \n",
    "#We represent this as a 2-D tensor of floating-point numbers, with a shape [None, 784]. \n",
    "#(Here None means that a dimension can be of any length.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what is the difference between a placeholder and a variable?\n",
    "\n",
    "We also need the weights and biases for our model. We could imagine treating these like additional inputs, but TensorFlow has an even better way to handle it: Variable. \n",
    "\n",
    "A **Variable** is a modifiable tensor that lives in TensorFlow's graph of interacting operations. It can be used and even modified by the computation. For machine learning applications, one generally has the model parameters be Variables.\n",
    "\n",
    "\n",
    "(So is it that a **variable** gets modified, as in forward/backward propagation while a **placeholder** will just hold the value of X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that W has a shape of [784, 10] because we want to multiply the 784-dimensional image vectors by it to produce 10-dimensional vectors of evidence for the difference classes. b has a shape of [10] so we can add it to the output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross-entropy\n",
    "https://colah.github.io/posts/2015-09-Visual-Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the source code, we don't use this formulation, \n",
    "\n",
    "because it is numerically unstable. \n",
    "\n",
    "Instead, we apply tf.nn.softmax_cross_entropy_with_logits on the unnormalized logits (e.g., we call softmax_cross_entropy_with_logits on tf.matmul(x, W) + b), because this more numerically stable function internally computes the softmax activation. \n",
    "\n",
    "In your code, consider using **tf.nn.softmax_cross_entropy_with_logits** instead.\n",
    "\n",
    "good reference for back-prop: http://colah.github.io/posts/2015-08-Backprop/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "https://www.tensorflow.org/api_guides/python/train#Optimizers\n",
    "\n",
    "The Optimizer base class provides methods to compute gradients for a loss and apply gradients to variables. A collection of subclasses implement classic optimization algorithms such as GradientDescent and Adagrad.\n",
    "\n",
    "You never instantiate the Optimizer class itself, but instead instantiate one of the subclasses.\n",
    "\n",
    "tf.train.Optimizer\n",
    "\n",
    "tf.train.GradientDescentOptimizer\n",
    "\n",
    "tf.train.AdadeltaOptimizer\n",
    "\n",
    "tf.train.AdagradOptimizer\n",
    "\n",
    "tf.train.AdagradDAOptimizer\n",
    "\n",
    "tf.train.MomentumOptimizer\n",
    "\n",
    "tf.train.AdamOptimizer\n",
    "\n",
    "tf.train.FtrlOptimizer\n",
    "\n",
    "tf.train.ProximalGradientDescentOptimizer\n",
    "\n",
    "tf.train.ProximalAdagradOptimizer\n",
    "\n",
    "tf.train.RMSPropOptimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "for _ in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Our Model\n",
    "\n",
    "\n",
    "Well, first let's figure out where we predicted the correct label. \n",
    "\n",
    "tf.argmax is an extremely useful function which gives you the index of the highest entry in a tensor along some axis. \n",
    "\n",
    "For example, tf.argmax(y,1) is the label our model thinks is most likely for each input, while tf.argmax(y_,1) is the correct label. \n",
    "\n",
    "We can use tf.equal to check if our prediction matches the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which gives us a list of Boolean values (did our prediction match the correct label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9111\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
